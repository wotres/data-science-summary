{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA(principal component analysis) 주성분 분석\n",
    "\n",
    "### PCA 이해 개념적으로 간단히 \n",
    "\n",
    "데이터의 차원을 줄이려고 하는데 원데이터의 성질을 가장 잘 보존하고자한다.\n",
    "\n",
    "이때 분산은 데이터들의 분포 특성을 가장 잘설명하며 이 분산의 방향이 가장 큰 방향벡터를 주성분이라고 함\n",
    "\n",
    "그리고 데이터를 가장 큰 방향벡터를 기준으로 투영(Projection)하여 변형(feature extraction) 하는것이 PCA이다.\n",
    "\n",
    "따라서 PCA 후에는 데이터 값이 달라지지만 기존 데이터의 분포 특성을 최대한 보존한다.\n",
    "\n",
    "첫번째 축을 찾은뒤 첫번째 축과 직교(orthogonal)하면서 분산이 가장 큰, 즉 가장 큰 방향벡터(주성분)를 찾은 뒤 축으로 하여 데이터를 투영하며\n",
    "\n",
    "이런 반복을 원하는 차원의 수만큼 반복한다. \n",
    "\n",
    "그런 뒤 투영된 값들을 새로운 feature로 변환시켜 사용한다.\n",
    "\n",
    "\n",
    "### PCA 이해 수학적으로 간단히 \n",
    "\n",
    "공분산 행렬은 데이터가 얼마나 함께 변화하는지를 나타낸다.\n",
    "\n",
    "공분산 행렬을 이용(Eigenvalue Decomposition)하면 고유 벡터(Eigenvector)와 고유값(Eigenvalue) 라는 것을 구할수 있다.\n",
    "\n",
    "Eigenvector 는 선형 변환을 하여도 방향이 보존되며 크기만 변하는 벡터이고, Eigenvalue 는 Eigenvector의 변하는 크기를 나타내며 이 고유값이 클수록 데이터의 경향성을 강하게 나타내고 있기떄문에 높을수록 좋다.\n",
    "\n",
    "그리고 구한 고유 벡터는 최대 분산의 방향에 해당한다.\n",
    "\n",
    "이 고유 벡터에 직교하는 그 다음 분산을 가장 잘 설명하는(고유값이 큰) eigenvector를 구하는 작업을 원하는 차원의 개수만큼 반복한다. \n",
    "\n",
    "그런 뒤 구한 고유벡터로 투영된 값들을 새로운 feature로 변환시켜 사용한다.\n",
    "\n",
    "참고로 보통 Eigenvalue의 분산 설명력의 비율이 90% 넘을때까지 벡터(차원의 수)들을 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9924428900898052\n",
      "[0.99244289 0.00755711]\n",
      "[0.99244289 1.        ]\n",
      "1\n",
      "0.007557109910194766\n",
      "[[-0.83849224 -0.54491354]\n",
      " [ 0.54491354 -0.83849224]]\n",
      "-0.5449135408239331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.38340578,  0.2935787 ],\n",
       "       [ 2.22189802, -0.25133484],\n",
       "       [ 3.6053038 ,  0.04224385],\n",
       "       [-1.38340578, -0.2935787 ],\n",
       "       [-2.22189802,  0.25133484],\n",
       "       [-3.6053038 , -0.04224385]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "# n_components: 투영할 축 개수, / svd_solver: 알고리즘\n",
    "pca = PCA(n_components=2, random_state=1234, svd_solver='auto')\n",
    "x_trans = pca.fit_transform(X)\n",
    "# 전체 분산 총량중 첫번째 주성분의 분산설명력의 비율\n",
    "print(pca.explained_variance_ratio_[0])\n",
    "\n",
    "# 위와 같이 설명 비율을 아래와 같은식\n",
    "pca_explained_variance_ratio_2 = pca.explained_variance_/sum(pca.explained_variance_)\n",
    "print(pca_explained_variance_ratio_2)\n",
    "\n",
    "# 누적 설명력\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# 90% 이상 누적 설명력을 가지는 차원의 최소 개수\n",
    "print(np.argmax(pca.explained_variance_ratio_.cumsum() >= 0.9) + 1)\n",
    "\n",
    "# 고유값 = Eigenvalue : 설명 정도\n",
    "# 두번째 주성분의 고유값\n",
    "print(pca.explained_variance_ratio_[1])\n",
    "\n",
    "# 고유벡터 = Eignevector : 축에 투영시 반영되는 방향 벡터값\n",
    "print(pca.components_)\n",
    "# 첫번째 주성분에 대한 2번째 변수의 사영계수값\n",
    "print(pca.components_[0,1])\n",
    "\n",
    "# 고유값과 고유 벡터로 투영되어 변환된 값\n",
    "x_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
