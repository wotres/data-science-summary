{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로지스틱 분류 LogisticRegression\n",
    "\n",
    "일반 선형 회귀 모형 ax + b => 0과 1로 구분되는 데이터셋은 회귀분석으로는 힘듬 \n",
    "\n",
    "=> 오즈비를 활용한 ln(p/1-p) = ax+b  => p = 시그모이드 함수 \n",
    "\n",
    "=> p 임계점에 따라 0과 1의 값으로 할당 가능 \n",
    "\n",
    "=> 즉 로지스틱 모델은 다양한 변수들을 종합하여 1,0 두가지로 확률을 예측하여 분류 하는 방법\n",
    "\n",
    "#### 오즈(Odds)란?\n",
    "\n",
    "성공 확률이 실패 확률에 비해 몇 배 더 높은가를 나타냄\n",
    "\n",
    "<img src='./images/odds.png' align='left'>\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "#### 로짓(logit)이란?\n",
    "\n",
    "오즈 비에 로그를 씌운값\n",
    "\n",
    "<img src='./images/logits.png' align='left'>\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "#### 로지스틱 유도 \n",
    "\n",
    "로짓과 회귀식의 연관성\n",
    "\n",
    "<img src='./images/logistic.png' align='left'>\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "이런뒤 p를 기준으로 우변 정렬하면 로지스틱 펑션이 나옴\n",
    "\n",
    "<img src='./images/logistic-function.png' align='left'>\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.129580575362974\n",
      "-3.129580575362974\n",
      "concavity error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn 이용\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=7777)\n",
    "train_y\n",
    "\n",
    "# C : Inverse of regularization, 절편없으면: fit_intercept=False\n",
    "lor = LogisticRegression(penalty='l2', C=100000, random_state=98709, solver='newton-cg')\n",
    "lor.fit(train_x, train_y)\n",
    "# 예측\n",
    "preds = lor.predict(test_x)\n",
    "\n",
    "# 예측 확률\n",
    "preds = lor.predict_proba(test_x)\n",
    "\n",
    "# logit 로짓 값들의 평균\n",
    "# observation별 logit 값의 평균\n",
    "# odds 는 실패에 비해 성공할 확률의 비 : p/(1-p)\n",
    "# odds 에 Log를 취한값이 바로 로짓 : Log(p)\n",
    "# 회귀분석의 y =ax +b 가 ln(p/1-p) = ax+b 가 된것 => 즉, 로지스틱 회귀분석은 일반회귀 함수를 로짓으로 변형한 분석법\n",
    "import math\n",
    "import numpy as np\n",
    "arr = []\n",
    "for pred in preds:\n",
    "  # 만약에 아주 확률이 1에 가까워 1 이 나온다면 최대한 1보다 작은 가장 큰값 입력\n",
    "  arr.append(math.log(float(pred[1])/float(1-pred[1])))\n",
    "print(np.mean(arr))\n",
    "# 또는\n",
    "print(np.mean(np.log(preds[:,1]/preds[:,0])))\n",
    "\n",
    "# 가장 영향이 큰(기준: Odds Ratio) 독립변수 => e 의 지수에 해당 변수가 위치하므로 np.exp 를 사용한다.\n",
    "cols = breast_cancer.feature_names\n",
    "print(cols[np.exp(lor.coef_).argmax()])\n",
    "\n",
    "# 변수에 대한 계수\n",
    "lor.coef_\n",
    "\n",
    "# 특정 변수에 대한 오즈비 => 로지스틱 회귀분석에서 오즈비는 np.exp(특정변수의 계수)\n",
    "np.exp(lor.coef_[0])\n",
    "\n",
    "# 평균면적이 1 증가할때 마다 유방암에 걸릴 확률이 몇배나 증가한다고 할 수 있는지를 구하라 => mean area 에 대한 오즈비를 구하라(Odds Ratio)\n",
    "# 오즈비는 1증가할떄마다 얼마나 증가하는 지를 보인다. 만약 0,2가 값으로 있다면 minmax를 하는것이 좋고, 3가지 값이 있다면 one-hot encoder 후 학습시킨뒤 비를 구하면 더 정확함\n",
    "i = cols.tolist().index('mean area')\n",
    "np.exp(lor.coef_[0][i])\n",
    "\n",
    "# 예측확률에 따른 예측치 결정(threshold or cut off value)\n",
    "pred_pro = np.where(preds[:,1]>0.5, 1, 0)\n",
    "pred_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.109327\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  341\n",
      "Model:                          Logit   Df Residuals:                      333\n",
      "Method:                           MLE   Df Model:                            7\n",
      "Date:                Sat, 19 Sep 2020   Pseudo R-squ.:                  0.8328\n",
      "Time:                        15:56:50   Log-Likelihood:                -37.281\n",
      "converged:                       True   LL-Null:                       -222.95\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.307e-76\n",
      "====================================================================================\n",
      "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept           45.0582     17.537      2.569      0.010      10.686      79.430\n",
      "mean_radius         -5.8674      6.405     -0.916      0.360     -18.422       6.687\n",
      "mean_texture        -0.4743      0.101     -4.707      0.000      -0.672      -0.277\n",
      "mean_perimeter       0.7831      0.808      0.970      0.332      -0.800       2.366\n",
      "mean_area           -0.0112      0.023     -0.494      0.621      -0.055       0.033\n",
      "mean_smoothness   -139.7322     37.109     -3.765      0.000    -212.465     -67.000\n",
      "mean_compactness   -15.3550     27.752     -0.553      0.580     -69.748      39.038\n",
      "mean_concavity     -22.8645      8.099     -2.823      0.005     -38.739      -6.990\n",
      "====================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.23 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>   341</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   333</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -37.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 19 Sep 2020</td> <th>  Deviance:          </th> <td>  74.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>15:56:50</td>     <th>  Pearson chi2:      </th>  <td>  450.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>9</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>   45.0582</td> <td>   17.537</td> <td>    2.569</td> <td> 0.010</td> <td>   10.686</td> <td>   79.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_radius</th>      <td>   -5.8674</td> <td>    6.405</td> <td>   -0.916</td> <td> 0.360</td> <td>  -18.422</td> <td>    6.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_texture</th>     <td>   -0.4743</td> <td>    0.101</td> <td>   -4.707</td> <td> 0.000</td> <td>   -0.672</td> <td>   -0.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_perimeter</th>   <td>    0.7831</td> <td>    0.808</td> <td>    0.970</td> <td> 0.332</td> <td>   -0.800</td> <td>    2.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_area</th>        <td>   -0.0112</td> <td>    0.023</td> <td>   -0.494</td> <td> 0.621</td> <td>   -0.055</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_smoothness</th>  <td> -139.7322</td> <td>   37.109</td> <td>   -3.765</td> <td> 0.000</td> <td> -212.465</td> <td>  -67.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_compactness</th> <td>  -15.3550</td> <td>   27.752</td> <td>   -0.553</td> <td> 0.580</td> <td>  -69.748</td> <td>   39.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_concavity</th>   <td>  -22.8645</td> <td>    8.099</td> <td>   -2.823</td> <td> 0.005</td> <td>  -38.739</td> <td>   -6.990</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  341\n",
       "Model:                            GLM   Df Residuals:                      333\n",
       "Model Family:                Binomial   Df Model:                            7\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -37.281\n",
       "Date:                Sat, 19 Sep 2020   Deviance:                       74.561\n",
       "Time:                        15:56:50   Pearson chi2:                     450.\n",
       "No. Iterations:                     9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept           45.0582     17.537      2.569      0.010      10.686      79.430\n",
       "mean_radius         -5.8674      6.405     -0.916      0.360     -18.422       6.687\n",
       "mean_texture        -0.4743      0.101     -4.707      0.000      -0.672      -0.277\n",
       "mean_perimeter       0.7831      0.808      0.970      0.332      -0.800       2.366\n",
       "mean_area           -0.0112      0.023     -0.494      0.621      -0.055       0.033\n",
       "mean_smoothness   -139.7322     37.109     -3.765      0.000    -212.465     -67.000\n",
       "mean_compactness   -15.3550     27.752     -0.553      0.580     -69.748      39.038\n",
       "mean_concavity     -22.8645      8.099     -2.823      0.005     -38.739      -6.990\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 통계적으로 유의한 독립변수 찾는법 => statsmodels 사용\n",
    "from sklearn import datasets\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "cols = [i.replace(' ' , '_') for i in breast_cancer.feature_names] + ['y']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=7777)\n",
    "train = np.hstack((train_x, train_y.reshape(-1,1)))\n",
    "test = np.hstack((test_x, test_y.reshape(-1,1)))\n",
    "train = pd.DataFrame(train, columns=cols)\n",
    "test = pd.DataFrame(test, columns=cols)\n",
    "\n",
    "# 20 개 넘어가면 잘안됨\n",
    "formula = 'y ~ mean_radius + mean_texture + mean_perimeter + mean_area + mean_smoothness + mean_compactness + mean_concavity'\n",
    "# train\n",
    "lor = sm.Logit.from_formula(formula, train).fit()\n",
    "# 요약\n",
    "print(lor.summary())\n",
    "# p_value 확인 => 0.05 이하이면 의미있는 변수\n",
    "lor.pvalues\n",
    "# 계수확인\n",
    "lor.params\n",
    "\n",
    "# 또 다른 방법. 일반선형 모델인 GLM(Generalize linear Model) 에서 이진 분류 하면 link function이 Logit 되어 Logistic regression 됨\n",
    "import statsmodels.formula.api as smf\n",
    "smf.glm(formula, train, family=sm.families.Binomial()).fit().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
