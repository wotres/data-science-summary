{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n",
      "[0. 0. 1.] [0.   0.75 1.  ] [2 1 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.83      0.88      0.83         6\n",
      "weighted avg       0.89      0.83      0.84         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix(분류 결과표): 타겟의 원래 클래스와 예측한 클래스가 일치하는지를 갯수로 센 결과를 표로 나타낸것\n",
    "# 정답 클래스를 행으로 / 예측 클래스를 열으로 나타냄\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2,0,2,2,0,1]\n",
    "y_pred = [0,0,2,2,0,2]\n",
    "confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 이중 특히 양성과 음성 2가지로 표현한것을 이진 분류결과표라고함 (Binary Confusion Matrix)\n",
    "#         예측양성  예측음성\n",
    "# 실제 양성   TP      FN\n",
    "# 실제 음성   FP      TN\n",
    "\n",
    "y_true = [1,0,1,1,0,1]\n",
    "y_pred = [0,0,1,1,0,1]\n",
    "confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "\n",
    "# Accuracy 정확도\n",
    "# 전체 샘플중 맞게 예측한 샘플수\n",
    "# (TP + TN) / Total\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Precision 정밀도\n",
    "# 양성이라고 판단한 샘플중 실제 양성 클래스인것 => 진짜라고 했는데 진짜 => 저격수(맞췄다 생각한거중 맞춘확률)\n",
    "# TP / (TP + FP)\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# recall (재현율) or sensitivity(민감도) or TPR(True Positive Rate)\n",
    "# 실제 양성중에 양성으로 예측된 비율 \n",
    "# TP / (TP + FN)\n",
    "from sklearn.metrics import recall_score\n",
    "# pos_label에 기준을 적어줌 1 or 'yes'\n",
    "recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# 위 값들은 실제로 1이 많아 그냥 모두 1이라고 예측할 경우 정확도가 높아지고 precision이 높아지는 경우가 있어 그나마 조금더 수치를 조정하여 만든것이 F Score \n",
    "# f1 score\n",
    "# (1+b^n) * (precision*recall) / (precision + recall)\n",
    "# 가중치가 1인 경우를 F1 Score 라고 함\n",
    "# 정밀도와 재현율의 가중 조화평균\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_true, y_pred, pos_label=1))\n",
    "\n",
    "# specificity(특이도)\n",
    "# 실제 음성중에 음성으로 예측된 비율\n",
    "# TN / (TN + FP)\n",
    "# 1- FPR\n",
    "\n",
    "# FPR (False Positive Rate)\n",
    "# 실제 음성인데 양성이라고 잘못 예측된 비율\n",
    "# FP / TN + FP\n",
    "# 1 - 특이도\n",
    "\n",
    "# TPR 과 FPR 은 서로 비례 => 조금만 1일꺼 같아도 1이라고 판정하는 모델은 TPR , FPR 함께상승. => 반대도 동일\n",
    "# 단점, 잘예측한 TPR 높이면 잘못예측한 FPR 또한 높아짐 => 따라서 FPR 대비 TPR 좋게 나오도록 => 시각화 한것이 ROC 커브\n",
    "\n",
    "# ROC curve (Receiver Operating Characteristics)\n",
    "# 장점 : 면적을 측정하여 TPR FPR 복합적으로 평가가능\n",
    "# 면적을 area under the curve:AUC 라고 하고 1에 가까울수록 성능이 좋고, 0.5에 가까울수록 성능이 나쁨\n",
    "# FPR , TPR , thresholds 해석 \n",
    "# case1 의사가 모든 환자들을 암 확률이 어느정도만 되어도(threshold가 낮음) 암 환자로 판정하면 TPR(실제 암이걸린환자를 암이라 판정) 과 FPR(암이걸리지 않았지만 암이라 판정) 이 함께 높아짐\n",
    "# => threshold 낮다 => TPR & FPR 비율 높음\n",
    "# case2 의사가 모든 환자들을 암 확률이 매우높아야만(threshold가 높음) 암 환자로 판정하면 TPR(실제 암이걸린환자를 암이라 판정) 과 FPR(암이걸리지 않았지만 암이라 판정) 이 함께 낮아진다.\n",
    "# => threshold 높다 => TPR & FPR 비율 낮음\n",
    "# 따라서 threshold 를 낮출수록 TPR & FPR 비율이 높아지고, threshold 를 높일수록 TPR & FPR 비율이 낮아진다. \n",
    "# 그리고 보통 FPR, TPR 순서로 확률 분포가 위치하여 threshold를 낮추면 TPR이 먼저 증가하고, threshold를 높이면 FPR이 먼저 사라진다.  \n",
    "from sklearn.metrics import roc_curve\n",
    "# thresholds : 임계값 => 의미: thresholds 의 이하의 값은 False, 이상은 True 로 판단.\n",
    "# 이 임계값에 따라 ROC 커브 위의 점 위치가 변화한다. \n",
    "# 휨 정도가 크다는것은 == 면적이 1에 가깝다는것은 두 클래스를 더욱 잘 구별 할수 있다는것이다. 즉, 아래 면적이 높을수록 좋다. \n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "print(fpr, tpr, thresholds)\n",
    "\n",
    "# 면적 구하는법\n",
    "# AUC : 아래 면적이 1에 가까울수록, 넓을 수록 좋은 모형\n",
    "from sklearn.metrics import auc\n",
    "auc(fpr, tpr)\n",
    "\n",
    "# 데이터 정답과 예측으로 바로 auc 구하는법\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# 성능평가 모두 구하기\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
