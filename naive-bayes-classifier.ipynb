{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier\n",
    "\n",
    "Bayes theorem 에서 유도됨\n",
    "\n",
    "베이즈 정리란? \n",
    "\n",
    "사후 확률(Posterior Probability) = 가능성(Likelihood) * 사전 확률(Class Prior Probability) / 특정 개체가 발생활 확률(Predictor Prio Probability) \n",
    "\n",
    "P(C|X) = P(X|C) * P(C) / P(X)\n",
    "\n",
    "그리고 X 를 형성하는 무수히 많은 특징(feature) 들이 있을떄 식은 다음과 같음\n",
    "\n",
    "P(C|X) = P(x1,x2,x3...xn|C) * P(C) / P(x1,x2,x3...xn)\n",
    "\n",
    "그리고 이때! feature 들은 서로 독립된 변수라는 가정! => Naive 단어 쓰인 이유중 하나\n",
    "\n",
    "독립이니 조건부 확률이 교집합으로 나타냄. \n",
    "\n",
    "P(C|X) = P(x1|C) * P(x2|C) * P(x3|C) * ... * P(xn|C) * P(C) / (P(x1)*P(x2)*...*P(xn)\n",
    "\n",
    "=> 각각의 특징들이 발생할 곱으로 쉽게 표현 가능\n",
    "\n",
    "메일(X)에 특정 단어 test(x1), free(x2) 가 포함될때 스팸(C)일 확률(P(C|x1,x2)) 은 아래와 같이 Naive 하게 구할 수 있음\n",
    "\n",
    "P(x1|C)*P(x2|C) *P(C) / (P(x1)*P(x2))\n",
    "\n",
    "가장 높은 확률 값을 제공하는 분류값을 찾는것이 주목적\n",
    "\n",
    "추가 : 가끔 없는 단어가 나오면 0이 되어버리는게 나이브 베이즈 알고리즘의 문제이다. 따라서 실제 코딩할때는 Smoothing이라는 기법을 쓴다 (간단히 해당빈도에 +1 등 조치를 하여 확률 0을 막는다.)\n",
    "\n",
    "3가지 나이브 베이즈 모형\n",
    "\n",
    "GaussianNB : 정규분포 / BernoulliNB : 베르누이분포 / MultinomialNB : 다항분포\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[3]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# 3가지 나이브 베이즈 모형\n",
    "# GaussianNB : 정규분포 / BernoulliNB : 베르누이분포 / MultinomialNB : 다항분포\n",
    "# 일반적인 연속값 속성을 가지는 데이터에 사용\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X, y)\n",
    "print(nb.predict([[-0.8,-1]]))\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "# 균일 분포의 난수 생성\n",
    "X = rng.randint(10, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 4, 5])\n",
    "# print(X)\n",
    "\n",
    "# 클래스 여러개(다항) 일때 주로 사용\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# alpha= additive laplace smoothing parameter : default 1.\n",
    "# 스무딩의 의미 : 없는 단어가 나오면 0이 되어버리는게 나이브 베이즈 알고리즘의 문제 => Smoothing 으로 특정 값을 더해줘 확률 0되는거 막음\n",
    "nb = MultinomialNB(alpha=2.0)\n",
    "nb.fit(X, y)\n",
    "print(nb.predict(X[2:3]))\n",
    "\n",
    "# 클래스가 1 , 0 또는 있다, 없다와 같이 이진속성일 경우 두개로 구분되는 binary 일 때 주로 사용.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# default 1.10\n",
    "nb = BernoulliNB(alpha=2.0)\n",
    "nb.fit(X, y)\n",
    "print(nb.predict(X[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
